sources:
  nginx_logs:
    type: file
    include:
      - "${NGINX_LOG_PATH:-/var/log/nginx/access.log}"
    read_from: end

transforms:
  # 1) 통합 파싱 + 에러 분류
  parse_and_classify_errors:
    inputs:
      - nginx_logs
    type: remap
    source: |
      # 파싱
      . = parse_regex!(
        string!(.message),
        r'^(?P<remote_addr>\S+) - (?P<remote_user>\S+) \[(?P<time_local>[^\]]+)\] "(?P<request>[^"]+)" (?P<status>\d+) (?P<body_bytes_sent>\d+) "(?P<http_referer>[^"]*)" "(?P<http_user_agent>[^"]*)" "(?P<http_x_forwarded_for>[^"]*)" "(?P<request_time>[^"]*)" "(?P<http_x_request_id>[^"]*)" "(?P<http_x_cluster>[^"]*)" "(?P<http_x_cluster_dbuser>[^"]*)" "(?P<target_backend>[^"]*)"$'
      )

      # 타입 변환
      .status = to_int!(.status)
      .body_bytes_sent = to_int!(.body_bytes_sent)
      .request_time = to_float!(.request_time)
      
      # 에러 분류 및 필터링
      if .status == 499 {
        .error_type = "client_timeout"
        .error_class = "499"
      } else if .status >= 500 && .status <= 599 {
        .error_type = "server_error" 
        .error_class = "5xx"
      } else if .request_time >= to_int!("${REQUEST_TIME_ALERT_THRESHOLD_SEC:-5}") { 
        .error_type = "SlowRequest"
        .error_class = "NginxRequestTimeOver_${REQUEST_TIME_ALERT_THRESHOLD_SEC:-5}sec"
      } else {
        # 에러가 아닌 로그는 버림
        abort
      }

  # 2) 통합 메트릭 생성
  unified_error_metrics:
    type: log_to_metric
    inputs:
      - parse_and_classify_errors
    metrics:
      - type: counter
        field: "status"
        name: "nginx_errors"
        tags:
          error_type: "{{ .error_type }}"
          error_class: "{{ .error_class }}"
          backend: "{{ .target_backend }}"

  # 3) 메트릭 집계
  error_aggregation:
    type: aggregate
    inputs:
      - unified_error_metrics
    interval_ms: 30000

  # 4) 메트릭을 로그로 변환
  metrics_to_logs:
    type: metric_to_log
    inputs: 
      - error_aggregation

  # 5) 알림 조건 필터링
  alert_filter:
    type: filter
    inputs:
      - metrics_to_logs
    condition: |
      # 임계값 설정
      threshold_499 = to_int!("${ALERT_499_THRESHOLD:-5}")
      threshold_5xx = to_int!("${ALERT_5XX_THRESHOLD:-10}")
      threshold_request_time = to_int!("${REQUEST_TIME_ALERT_THRESHOLD:-5}")

      # 현재 에러 개수
      error_count = to_int!(.counter.value)
      
      # nginx 에러 메트릭인지 확인
      is_nginx_error = .name == "nginx_errors"
      
      # 각 에러 타입별 임계값 확인
      is_request_time_alert = .tags.error_type == "SlowRequest" && error_count >= threshold_request_time 
      is_client_timeout_alert = .tags.error_type == "client_timeout" && error_count >= threshold_499
      is_server_error_alert = .tags.error_type == "server_error" && error_count >= threshold_5xx
      
      # 최종 조건: nginx 에러이면서 임계값을 초과한 경우
      is_nginx_error && (is_client_timeout_alert || is_server_error_alert || is_request_time_alert)

  # 6) 공통 알림 데이터 준비
  prepare_alert_data:
    type: remap
    inputs:
      - alert_filter
    source: |
      # 공통 데이터 추출
      backend_full = to_string!(.tags.backend)
      .backend = replace(backend_full, r'backend(\d+)', "$$1")
      .count = to_string(to_int!(.counter.value))
      .error_class = to_string!(.tags.error_class)
      
      # 동적으로 서버 정보 가져오기
      .server_hostname = get_hostname() ?? "unknown"
      
      # hostname에서 cluster 정보 파싱 시도
      hostname_parsed = parse_regex(.server_hostname, r'^(?P<cluster_prefix>[^-]+)-(?P<cluster_group>[^-]+)-(?P<server_type>.+)$') ?? {}
      
      # 파싱 성공 여부에 따라 설정
      .cluster_prefix = if exists(hostname_parsed.cluster_prefix) { hostname_parsed.cluster_prefix } else { "${CLUSTER_PREFIX:-vc33}" }
      .cluster_group = if exists(hostname_parsed.cluster_group) { hostname_parsed.cluster_group } else { "${CLUSTER_GROUP:-g1}" }
      .server_type = if exists(hostname_parsed.server_type) { hostname_parsed.server_type } else { "${SERVER_TYPE:-was1}" }
      .server_ip = get_env_var("SERVER_IP") ?? .server_hostname
      .interval_sec = to_int!("${AGGREGATION_INTERVAL_MS:-30000}") / 1000
      
      # 기본 메시지 생성
      .base_message = "⚠️ Vector 알림: [" + .cluster_group + "] [*" + .cluster_prefix + "-" + .backend + "*] " + .server_type + " (" + .server_ip + ")] " + to_string(.interval_sec) + "초 사이에 " + .count + "개의 " + .error_class + " 에러가 발생했습니다."

  # 7) Slack 형식 포맷팅
  format_slack_message:
    type: remap
    inputs:
      - prepare_alert_data
    source: |
      . = { 
        "message": {
          "text": .base_message,
          "type": "mrkdwn"
        }
      }

  # 8) 알람서버 형식 포맷팅  
  format_alarm_server_message:
    type: remap
    inputs:
      - prepare_alert_data
    source: |
      . = { 
        "message": {
          "from": "vector",
          "severity": "WARNING",
          "text": .base_message,
          "type": "mrkdwn"
        }
      }

sinks:
  # 디버그 출력 - 알림 확인용
  # parse_and_classify:
  #   type: console
  #   inputs:
  #     - parse_and_classify_errors
  #   encoding:
  #     codec: json

  debug_alerts:
    type: console
    inputs:
      - format_alarm_server_message
    encoding:
      codec: json

  # 알람서버 알림
  alarm_server_alert:
    type: http
    inputs: 
      - format_alarm_server_message
    uri: "${ARLARM_WEBHOOK_URL:-https://hook.trustnhope.kr/alaram}"
    method: post
    encoding:
      codec: text
    batch:
      timeout_secs: 1
      max_events: 1
    request:
      headers:
        Content-Type: "application/json"

  # Slack 알림
  slack_alert:
    type: http
    inputs: 
      - format_slack_message
    uri: "${SLACK_WEBHOOK_URL:-https://hooks.slack.com/services/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}"
    method: post
    encoding:
      codec: text
    batch:
      timeout_secs: 1
      max_events: 1
    request:
      headers:
        Content-Type: "application/json"